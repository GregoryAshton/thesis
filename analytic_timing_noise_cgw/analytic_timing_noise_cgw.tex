\documentclass[../full_thesis/full_thesis.tex]{subfiles}

% Default image directory
\newcommand{\thisdir}{../analytic_timing_noise_cgw}
\graphicspath{{\thisdir/img/}}

\begin{document}

\meta{ It's unclear at this time what causes timing noise, but from
Chapter.~\ref{sec: timing noise in cgw} we know that it may pose a problem for
continuous GW searches if the phase evolution is effected by timing noise and
this is not included in the search templates. In addition to searches for
isolated neutron stars, many searches \citep[see for
example][]{ligo2015scox1,leaci2015,ScoX1:MDC1} have been performed for GWs from
low-mass X-ray binary systems (LMXBs). These searches experience similar
difficulties due to a stronger form of timing noise known as `spin-wandering'.
This issue was investigated by \citet{watts2008} who quantifed the effect using
a decoherence time $T_{\textrm{decoh}}$, after which the Taylor expansion can
no longer track the phase, defined by $T_{\textrm{decoh}}^{2} \dot{\nu} = 1.$
They then estimated a worst-case decoherence time by using the maximimal
spin-up rate $\dot{\nu}$ due to the accretion torque and found that for some
sources such as the LMXB Scorpius-X1, the decoherence time can be short as $\sim 1$~week.

In this chapter, we will present some preliminary calculations and results
related to modelling timing noise in a continuous GW using a random walk model.
Unlike the results of Chapter.~\ref{sec: timing noise in cgw}, which used an
empirical description of timing noise given by the Crab ephemeris, the results
derived here can be applied to any search in which it is thought the signal may
undergo a random walk. In this sense, it is equivalent to Chapter~\ref{sec:
glitches in cgw} in that the ultimate aim is to use estimate the risk faced by
various searches by inferring from the observed pulsar population.  As given
here, the task in incomplete and so we will present our calculations and
some preliminary results.

Recent observations by \citet{Hobbs2010} suggest that a random walk model does
not capture the physics of timing noise in isolated radio pulsars and hence is
not a useful way to infer neutron star physics. Nevertheless, the random walk
model remains a practical empirical model; in this section then, we use it as
such without requiring it to have any deeper substantive meaning for what
causes timing noise. In the same way, the random walk model can also be applied
to LMXBs where the amount of spin-wandering could be inferred from fluctuations
in the luminosity. From this we intend in the future to update the estimates by
\citet{watts2008} to estimate the mismatch for various searches.

This chapter is organised in the following way. In Section~\ref{sec: defining a
random walk} we will define the random walk as a piecewise Taylor expansion in
which the difference between the signal and template is initially zero. We will
then calculate the mismatch for this simple description in Section~\ref{sec:
random walkm models part I}. However, setting the difference between the signal
and template to zero initially does not minimise the mismatch, so in
Section~\ref{sec: random walk models part II} we discuss a pragmatic to
estimate the minimised mismatch; for each predicted mismatch we veryify our
calculations using Monte-Carlo like numerical simulations. In Section~\ref{sec:
application to the crab} we interpret the Crab ephemeris (introduced in
Chapter~\ref{sec: timing noise cgw}) as a random walk and then use this to
predict the mismatch as a function of observation time. Finally in
Section~\ref{sec: other pulsars} we will use a fitting formulae to predict the
worst-case mismatch in some recent gravitational wave searches.

}

\section{Defining a random walk}
\label{sec: defining a random walk}
To calculate the fully-coherent mismatch, we will model the random walk as a
zero-mean Gaussian walk in the phase, frequency, and spin-down which occurs at
$\Nsd$ fixed time intervals, $\Delta T$ such that the total observation time is
$\Tobs = \Nsd \Delta T$.
This allows us to write the signal as 
a piecewise Taylor expansion with $N$ subdomans. Choosing fixed time intervals appears to
introduce an additional timescale not usually present in random walk models.
However, as shown later in Section~\ref{sec: crab RW} this is consistent with a
large number of unresolved events which
are measured over a fixed timescale.

In this model, the spin-down rate will be the highest order term which
undergoes a random walk. Recalling that $\Delta \dot{f}_i$ (defined in
Eqn.~\eqref{eqn: Delta Phi}) is the difference between the signal and template
in the $i^{th}$ subdomain, we may define the jump in this difference between
the $i$ and $i-1$ subdomains as $\tn \fdot_i$, such that
\begin{equation}
\Delta \fdot_{i} - \Delta\fdot_{i-1} = \tn \fdot_{i} \sim \mathcal{N}(0, \sigS),
\end{equation}
where $\mathcal{N}$ denotes the normal distribution and we have defined $\sigS$
as the standard-deviation of the step sizes in the spin-down rate. The residual
between parameter space offsets is denoted by $\tn$ which is normally
distributed. Rearranging this gives an expression for the offset in the
$i^{th}$ subdomain, by induction we can also write down the $i-1$ term
\begin{align}
\Delta\fdot_{i} &  = \tn\fdot_{i} + \Delta\fdot_{i-1}  \\
\Delta\fdot_{i-1} &  = \tn\fdot_{i-1} + \Delta\fdot_{i-2}  .
\end{align}
Let us set the initial difference between the signal and template as zero such
that $\Delta\dot{f}_0=0$, that is we start each random walk from the origin
(we will return to this point in Section~\ref{sec: random walk models part
II}). Then as each step proceeds from the previous step, we have that
\begin{equation} 
\Delta\fdot_{i} = \s{j=1}{i}\tn\fdot_{j}.
\label{eqn: delta fdot n} 
\end{equation}

To illustrate this, in Figure~\ref{fig: Illustration fdot int} we plot an example
of a random walk in spin-down as given by Eqn.~\eqref{eqn: delta fdot n}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{Illustration_F1_int}
\caption{An example of a random walk in the spin-down rate, 
Eqn.~\eqref{eqn: delta fdot n}. The filled green blocks indicate the
summation defined in Eqn.~\eqref{eqn: f offset induced} required to
calculate the induced change in frequency at $t^{i}$ due to the random walk in
spin-down rate.}
\label{fig: Illustration fdot int}
\end{figure}

If we want to model a random walk in the phase, frequency, and spin-down rate
concurrently, then we must consider the effect that a
random walk in spin-down will have on the frequency and phase. For example, if we
increase the spin-down rate for a period of time, then we would expect the frequency
to decrease at a greater rate during this period. In our discreet model, it is
not possible to dynamically change the frequency during a single subdomain.
However, we can approximate this by updating the frequency in the next
subdomain with the induced frequency offset due to the spin-down in previous
subdomains. This must be done for the induced effect from the random walk in spin-down rate on
the phase and frequency, and for the induced effect from the random walk in frequency
on the phase. There is induced effect for the spin-down rate: the effect only
propagates to lower order terms.

Because the random walk is discreet and constant in any given subdomain, we can
calculate the offset in the lower order terms from a Taylor expansion. The
total offset at the  $i^{th}$ reference time is then given by the summation of
the offset caused by all higher order terms up to that reference time. The
reference times can be arbitrarily chosen, but setting each to start at the
beginning of the subdomain simplifies the calculation. 
The frequency offset induced by the spin-down can be
calculated using a Taylor expansion
\begin{equation}
\Delta \f_{i} = \s{j=1}{i-1}\Delta\fdot_{j} \dT .
\label{eqn: f offset induced} 
\end{equation}
This can be though of as the integration of the spin-down up to the $i^{th}$
reference time and is illustrated by the green blocks in Figure~\ref{fig:
Illustration fdot int}. 

Since we want to consider random walks in all three parameters we now add in a
random walk in frequency. Each step is independent of the induced effect from
the spin-down and is given by \mbox{$\tn \f_{i} \sim \mathcal{N}(0, \sigF)$}. The two
effects will sum linearly such that the frequency offset is
\begin{equation}
\Delta \f_{i} = \s{j=1}{i}\tn \f_{j} + \s{j=1}{i-1}\Delta\fdot_{j} \dT.
\label{eqn: f offset} 
\end{equation}

By a similar process we can calculate the induced effect of the frequency and
spin-down on the phase. Including the random walk in the phase
for which $\delta \phi_i \sim \mathcal{N}(0, \sigP)$, the phase offset is given by
\begin{equation}
\Delta\phi_{i}  =  \s{j=1}{i}\tn \phi_{j} 
+ 2\pi\left(\s{j=1}{i-1}\Delta \f_{j}\dT 
+ \frac{1}{2}\s{j=1}{i-1}\Delta \fdot_{j}\dT^{2}\right).
\label{eqn: phi offset}
\end{equation}

%Equations \eqref{eqn: f offset} and \eqref{eqn: phi offset} will allows us to
%construct the parameter space offsets in all three terms from the distibutions
%$\tn \phi_i$, $\tn \phi_i$, and $\tn \phi_i$.


\section{Random walk models: a simple treatment}
\label{sec: random walk models part I}
We will now calculate the mismatch for a fully-coherent search given the
random walk in phase, frequency, and spin-down rate defined in the previous
section.

Let us begin by expanding the metric-mismatch summation from Eqn.~\eqref{eqn:
mismatch}. Recalling that Greek indices label the parameter components and
Roman indices label the subdomain, then writing the summations explicitly, we
have
\begin{align}
\mutilde & = g_{\alpha\beta i j}\dl^{\alpha i}\dl^{\beta j}  \\
&=\s{i=1}{\Nsd}\s{j=1}{\Nsd}g_{\alpha\beta i j}\dl^{\alpha i}\dl^{\beta j}  \\
&= \s{i=1}{\Nsd}g_{\alpha\beta i i}\dl^{\alpha i}\dl^{\beta i}
+ \s{i=1}{\Nsd} \s{\substack{j=1\\ j \ne i}}{\Nsd} g_{\alpha \beta ij}\dl^{\alpha i}\dl^{\beta j}.
\end{align}
The summation has been intentionally split into terms for which the two
subdomains are the same and those for which they are different. The metric when
the reference time is at the beginning of each subdomain is given by
Eqn.~\eqref{eqn: metric equal subdomains tref 0}. By considering the metric for
the two cases, we can write the two distinct components as
\begin{equation}
g_{\alpha\beta ij} = \left\{
\begin{array}{cc}
g_{\alpha\beta}^{\mathrm{E}} & \textrm{ if } i =j \\
g_{\alpha\beta}^{\mathrm{NE}} & \textrm{ if } i  \ne j
\end{array}\right.  .
\end{equation}
Then the fully-coherent metric-mismatch can be calculated from
\begin{align}
\mutilde &= \s{i=1}{\Nsd}g_{\alpha\beta}^{\mathrm{E}}\dl^{\alpha i}\dl^{\beta i}
+ 2\s{i=1}{\Nsd} \s{j=1}{i-1} g_{\alpha \beta}^{\mathrm{NE}}\dl^{\alpha i}\dl^{\beta j} .
\label{eqn: mismatch sep}
\end{align}

\subsection{Writing the parameter offsets in terms of normal distributions}
Equations~\eqref{eqn: f offset} and \eqref{eqn: phi offset} give the offsets as
functions of the offsets in higher order parameters. In order to calculate
statistical values, we now write these in terms of the normal distributions
from which the random walks are constructed.  Substituting Eqn.~\eqref{eqn:
delta fdot n} into Eqn.~\eqref{eqn: f offset} and using the indentity defined
in Eqn.~\eqref{eqn: SI 1} of Appendix~\ref{sec: summation identities}, we have
\begin{align}
\Delta \f_{i}  & = \s{j=1}{i}\tn \f_{j}
+ \s{j=1}{i-1}\s{k=1}{j}\tn \fdot_{k} \dT ,  \\
& = \s{j=1}{i}\tn \f_{j}
+ \s{j=1}{i-1}(i-j)\tn \fdot_{j} \dT .
\label{eqn: delta f n}
\end{align}
Similarly, substituting this equation into Eqn.~\eqref{eqn: phi offset} and
using both Eqn.~\eqref{eqn: SI 1} and Eqn.~\eqref{eqn: SI 2} from
Appendix~\ref{sec: summation identities}, we have
\begin{align}
\begin{split}
\Delta\phi_{i} & = \s{j=1}{i}\tn \phi_{j}
+ 2\pi \left(\s{j=1}{i-1}\Delta\f_{j}\dT
+ \frac{1}{2}\s{j=1}{i-1}\Delta\fdot_{j}\dT^{2}\right) \\
& = \s{j=1}{i}\tn \phi_{j} + 2\pi\left(\s{j=1}{i-1}\left(\s{k=1}{j}\tn\f_{k}
+ \s{k=1}{j-1}(j-k)\tn\fdot_{k}\dT\right)\dT
 + \frac{1}{2}\s{j=1}{i-1}\s{k=1}{j}\Delta\fdot_{k}\dT^{2} \right)  \\
& = \s{j=1}{i}\tn \phi_{j} + 2\pi\left(\s{j=1}{i-1}(i-j)\tn\f_{j}\dT
 + \s{j=1}{i-1}\s{k=1}{j-1}(j-k)\tn\fdot_{k}\dT^{2}
 + \frac{1}{2}\s{j=1}{i-1}(i-j)\Delta\fdot_{j}\dT^{2} \right)  \\
& = \s{j=1}{i}\tn \phi_{j} + 2\pi\left(\s{j=1}{i-1}(i-j)\tn\f_{j}\dT
 + \frac{1}{2}\s{j=1}{i-1}\left(\left(i-j\right)\left(i-j-1)\right)
 + (i-j)\right)\tn\fdot_{j}\dT^{2}\right)  \\
& = \s{j=1}{i}\tn \phi_{j} + 2\pi\left(\s{j=1}{i-1}(i-j)\tn\f_{j}\dT
 + \frac{1}{2}\s{j=1}{i-1}(i-j)^{2}\tn\fdot_{j}\dT^{2}\right).
\end{split}
\label{eqn: delta phi n}
\end{align}
This result can be interpretted as the accumulated phase over a time $i\Delta
T$ due to a random walk in the phase, frequency, and spin-down rate.

\subsection{Taking the expectation}

In Eqn.~\eqref{eqn: delta fdot n}, Eqn.~\eqref{eqn: delta f n},
Eqn.~\eqref{eqn: delta phi n} we have written the parameter space offsets
(which are to be used in calculating the mismatch) purely in terms of the
random walk distributions $\tn \phi_i$, $\tn \f_i$, and $\tn \fdot_i$. We can
calculate the mismatch exactly given a set of random walk jumps by inserting
these into Eqn.~\eqref{eqn: mismatch sep}. However, since we are dealing with
statistical quantities, we can instead infer the behaviour of the mismatch
under the random walk by taking an expectation.

Inserting Eqn.~\eqref{eqn: delta fdot n}, Eqn.~\eqref{eqn: delta f n},
Eqn.~\eqref{eqn: delta phi n}  in Eqn.~\eqref{eqn: mismatch sep} yields a
number of terms with all the permutations of two terms from $[\tn \phi, \tn \f,
\tn \fdot]$. Taking the expectation, all the cross-correlated terms, such as
$\tn\phi_{i}\tn \fdot$, will have an expectation of zero since the steps of the
random walk are independent. The only non-vanishing terms are given by
\begin{align}
E[\tn\phi_{i}\tn\phi_{j}] &= \delta_{ij}\sigP, &
E[\tn\f_{i}\tn\f_{j}] &= \delta_{ij}\sigF,&
E[\tn\fdot_{i}\tn\fdot_{j}] &= \delta_{ij}\sigS,
\end{align}
After some simplification we find that the mismatch is given by
\begin{align}
\begin{split}
E[\mutilde]   = &  \frac{A_{\phi}}{6} \left(\Nsd - \frac{1}{\Nsd}\right)
+ \frac{\pi^{2} A_{{f}}}{30}\left(4 \Nsd^{3} + 5 \Nsd^{2} + \frac{1}{\Nsd}\right)\\
 & +  \frac{\pi^{2} A_{{\dot{f}}}}{3780} \left(66 \Nsd^{5} - 21 \Nsd^{3} + 105 \Nsd^{2}
 + 217 \Nsd + 63 - \frac{94}{\Nsd}\right),
\end{split}
\label{eqn: expectation}
\end{align}
where
\begin{equation}
	A_{\phi} = \sigP \;\;\;\;\;
    A_{\f} = \sigF\Delta T^{2} \;\;\;\;\;
    A_{\fdot} = \sigS\Delta T^{4},
\end{equation}
define three dimensionless `activity parameters'.

Recalling that $\Nsd=\Tobs/\Delta T$, Eqn.~\eqref{eqn: expectation} makes
predictions for the leading order
scaling of the three random walks with the observation period
\begin{equation}
E[\mutilde]_{\mathrm{PN}} \sim \sigP \frac{\Tobs}{\Delta T}, \hspace{10mm}
E[\mutilde]_{\mathrm{FN}} \sim \sigF \frac{\Tobs^{3}}{\Delta T}, \hspace{10mm}
E[\mutilde]_{\mathrm{SN}} \sim \sigS \frac{\Tobs^{5}}{\Delta T}.
\label{eqn: scalings}
\end{equation}
We note here the exact relation to the scalings of the variance of the
root-mean phase residual as calculated by \citet{Cordes1980} and given in
Eqn.~\eqref{eqn: S calc}. We will return to this later in Sec.~\ref{sec: crab RW}.

%\meta{MOVE
%These results are a function both of the observation span $\Tobs$ and the
%random walk model parameters $\sigP, \sigF, \sigS$ and $\Delta T$. In
%Appendix~\ref{sec: crab RW}, we showed
%that for a compound Poisson process random walk, the variance after a duration
%$\Delta T$ scaled as $\sigma^{2} \propto \Delta T$. This means that the leading
%order scalings in Eqn.~\eqref{eqn: scalings} are insensitive to how the random
%walk is parameterised: changing $\Delta T$ produces a corresponding change in
%$\sigma^{2}$ such that the leading order mismatch remains the same for a fixed
%observation time.
%}

\subsection{Verifying the results}

We can observe the leading order scaling of Eqn.~\eqref{eqn: scalings}
directly and verify the predictions made by Eqn.~\eqref{eqn: expectation} by
comparing with exact numerical results. That is, using the signal injection and
recovery tools developed in Section~\ref{sec: narrow-band method} of
Chapter.~\ref{sec: timing noise in cgw} we simulate signals undergoing a random
walk and calculate the corresponding mismatch (no minimisation step is done
here, this is discussed in the next section). In particular, we perform three
Monte Carlo studies for a random walk in the phase, frequency, and spin-down
rate and in each case compare the simulated results with the analytic
prediction. The results are shown in Figure~\ref{fig: rw I} and demonstrate good
agreement between the simulation means and the prediction of Eqn.~\eqref{eqn:
expectation}.

\begin{figure}[ht]
\centering
\subfloat[Random walk in phase]{\includegraphics[width=0.5\textwidth]{ExpectationPhase}}
\subfloat[Random walk in frequency]{\includegraphics[width=0.5\textwidth]{ExpectationFrequency}}\\ \subfloat[Random walk in spin-down]{\includegraphics[width=0.5\textwidth]{ExpectationSpindown}}
\caption{A comparison of Monte Carlo numerical simulated mismatch with the prediction
of Eqn.~\eqref{eqn: expectation} for a random walk in the phase, frequency,
and spin-down rate.}
\label{fig: rw I}
\end{figure}

\section{Random walk models: minimising the mismatch} 
\label{sec: random walk models part II}
In Section~\ref{sec: defining a random walk} we have defined a random walk
model for which we subsequently calculated the fully-coherent mismatch in
Section~\ref{sec: random walk models part I}. However, this is a special case
in which the random walk for each parameter offset (the difference between the
signal and the template) begins at the origin and then grows with time. It is
the signal which undergoes a random walk, so in this case we have set the
template to exactly match the signal and $t=0$. However, one could imagine choosing
the template in a different way which would reduce the overall mismatch; as such
Eqn.~\eqref{eqn: expectation} may overestimate the mismatch. The proper thing to
do is to minimise the mismatch with respect to the
template parameters~$\lt^{\alpha}$ which are implicitly in the calculation of
Section~\ref{sec: random walk models part I} through
\begin{align}
\Delta \lambda^{\alpha i} = \ls^{\alpha i} - \lt^{\alpha i},
\end{align}
as first defined in Section~\ref{sec: generalising the metric-mismatch}.

Ideally, we would like to repeat the calculation leading to Eqn.~\eqref{eqn:
expectation} minimising the mismatch with respect to the template parameters.
However, this calculation has not yet been peformed so a practical alternative
method which we will use here is to begin with the random walk starting at the
origin, as defined in Section~\ref{sec: defining a random walk}, and then fit and
remove a polynomial of degree $k$. This leaves us with a residual random walk for
which we then compute the mismatch. We will then verify that this captures the
essential features of minimising the mismatch by comparing with numerical
simulations in which the exact mismatch is minimised numerically.

In Appendix~\ref{sec: least squares minimisation of a random walk}, we
introduce the basic tools of least squares fitting and removing a polynomial of
degree $k$ to a generic random walk. In the following sections, we will
calculate the minimised mismatch for random walks in the phase or frequency; we
have not yet calculated the corresponding result for mixtures or random walks
in the spin-down rate. We have a choice in the degree of polynomial to fit and
remove. Since most searches minimise the mismatch with respect to the template
frequency $f_\textrm{t}$ and spin-down rate $\dot{f}_{\textrm{t}}$, this is
equivalent to fitting and removing a $k=2$ polynomial to the phase residual.

\subsection{Random walk in the phase}
\label{sec: minimised rw in phase}
We begin with the simplest case of a random walk in phase, for which we have
\begin{equation}
\Delta\phi_{i} = \s{j=1}{i}\mathcal{N}(0, \sigP).
\end{equation}
Then, as shown in Eqn.~\eqref{eqn: E yiyi} of Appendix~\ref{sec: least squares
minimisation of a random walk}, we have that
\begin{equation}
E[\Delta\phi_{i} \Delta\phi_{j}] = \sigP \min(i, j).
\end{equation}

Then, we define the residual difference between the signal and template
after fitting and removing a $2^{nd}$ order polynomial, $\hat{y}_i^{(2)}$, as
\begin{align}
\Delta^{(2)}\phi_i = \Delta\phi_i - \hat{y}_i^{(2)}.
\label{eqn: D2phi}
\end{align}
Note that the superscript `(2)' indicates the degree of polynomial and by
$\Delta^{(2)}\phi_i$ we mean the residual difference between the signal and
template after fitting and removing the polynomial.

We set the difference between the signal and template in all other parameters
to zero such that the mismatch for a random walk in the residual phase is
therefore
\begin{align}
\mutilde & = g_{0 0 i j} \Delta^{(2)}\phi^{i}\Delta^{(2)}\phi^{j} \\
& = \s{i=1}{\Nsd}g_{00}^{E} \Delta^{(2)}\phi_{i}\Delta^{(2)}\phi_{i}
+ 2 \s{i=1}{\Nsd}\s{j=1}{i-1}g_{00}^{NE}\Delta^{(2)}\phi_{i}\Delta^{(2)}\phi_{j}.
\label{eqn: 4202540871}
\end{align}
To calculate the expectation of the mismatch, we need to evaluate the
expectation of
\begin{align}
\Delta^{(2)}\phi_{i}\Delta^{(2)}\phi_{i} = & \left(\Delta\phi_{i}
- \s{k=1}{\Nsd}\CT_{ik}\Delta\phi_{k}\right)
 \left(\Delta\phi_{j} - \s{l=1}{\Nsd}\CT_{jl}\Delta\phi_{l}\right) \\
= & \Delta\phi_{i}\Delta\phi_{j} -
\left(\s{k=1}{\Nsd}\CT_{ik} \Delta\phi_{j}\Delta\phi_{k}
+ \s{l=1}{\Nsd}\CT_{jl}\Delta\phi_{i}\Delta\phi_{l}\right) \nonumber \\
& +
\s{k=1}{\Nsd}\s{l=1}{\Nsd}\CT_{ik}\CT_{jl} \Delta\phi_{k}\Delta\phi_{l},
\end{align}
where $\CT_{ij}$ is defined in Eqn.~\ref{eqn: C_2} and Eqn.~\ref{eqn: MC_2}
of Appendix~\ref{sec: least squares minimisation of a random walk} and we have
replaced $\Delta x$ with the time $\dT$. Then taking the expectation
\begin{align}
\E{\Delta^{(2)}\phi_{i}\Delta^{(2)}\phi_{i}} & =
%E\left[\Delta\phi_{i}\Delta\phi_{j}\right] -
%\left(\s{k=1}{\Nsd}E[\Delta\phi_{j}\Delta\phi_{k}]
%+ \s{l=1}{\Nsd}E[\Delta\phi_{i}\Delta\phi_{l}]\right) +
%\s{k=1}{\Nsd}\s{l=1}{\Nsd}E[\Delta\phi_{k}\Delta\phi_{l}]\\
%& = 
\sigma^{2}_{\phi} \left(\min(i, j) - \left(\s{k=1}{\Nsd}\CT_{ik} \min(j, k)
+ \s{l=1}{\Nsd}\CT_{jl}\min(i, l) \right)\right. \notag \\
& \hspace{13mm} \left. + \s{k=1}{\Nsd}\s{l=1}{\Nsd}\CT_{ik}\CT_{jl}\min(k, l)\right).
\label{eqn: expected mismatch dP0idP0j_k2}
\end{align}
Using symbolic mathematics packages we
calculate an analytic expression which is a function of $\dT, i, j$ and $\Nsd$.
Inserting this into Eqn.~\eqref{eqn: 4202540871} and simplifying we find that
\begin{align}
E[\mutilde]  & = \s{i=1}{\Nsd}g_{00}^{E} E\left[\Delta^{(2)}\phi_{i}\Delta^{(2)}\phi_{i}\right]
+ 2 \s{i=1}{\Nsd}\s{j=1}{i-1}g_{00}^{NE}E\left[\Delta^{(2)}\phi_{i}\Delta^{(2)}\phi_{j}\right]  \\
& = \frac{1}{70}\sigP\left(3N - \frac{27}{\Nsd}\right).
\label{eqn: Expected mismatch RW in phase k2}
\end{align}
This expression can be compared to Eqn.~\eqref{eqn: expectation} ignoring the
effect of the random walk in spin-down rate. Notably, we retain the same
leading order scaling of $\Nsd$, but the overall coefficient is decreased.
Rearranging the expression in the bracket demonstrates the mismatch is negative
or zero for $1 \ge \Nsd \ge 3$: this is a reflection of the minimum number of
points needed in order to perform the quadratic fit. This is shown later in
Sec.~\ref{sec: appendix conclusions} for the simpler case of fitting and
removing a polynomial from a generic random walk.

\subsection{Random walk in the frequency}

For a random walk in the frequency we have an added complexity caused by the
effect the frequency offsets induces in the phase. For the frequency offset we
have
\begin{align}
\Delta f_{i} &= \s{j=1}{i}\mathcal{N}(0, \sigF).
\end{align}
Recalling that we set the reference time at the beginning of each subdomain,
then as in Section~\ref{sec: defining a random walk}, the induced phase offset is
\begin{align}
\Delta\phi_{i} &=2\pi \s{j=1}{i-1}\Delta f_{j}\dT \\
 & = 2\pi\dT \s{j=1}{i-1}\s{k=1}{j}\mathcal{N}(0, \sigF) \\
& = 2\pi\dT \s{j=1}{i}(i-j)\mathcal{N}(0, \sigF).
\label{eqn: P2F}
\end{align}
Note that we do not include a random walk in the phase here.

Then we calculate the expected values of combinations of the parameter space
offsets
\begin{align}
E[\Delta\f_{i}\Delta\f_{j}] & = \sigF \min(i, j), \label{eqn: E1} \\
E[\Delta\phi_{i}\Delta\f_{j}] & = 2 \pi \dT \sigF \s{k=1}{\min(i, j)}(i-k), \label{eqn: E2}\\
E[\Delta\phi_{i}\Delta\phi_{j}] & =
\left(2\pi\dT\right)^{2}\sigF \s{k=1}{\min(i, j)}(i-k)(j-k).
\label{eqn: E3}
\end{align}

In Eqn.~\eqref{eqn: D2phi}, we defined the residual difference between the signal
and template phase after fitting and removing a scond order polynomial. The
second order polynomial was chosen to model the effect of minimising over the
template frequency and frequency derivative. Let us now define
\begin{align}
\Delta^{(1)}f_i = \Delta f_i - \hat{y}^{(1)},
\label{eqn: D2f}
\end{align}
as the residual difference between the signal and template frequency after
fitting and removing a first order polynomial. In this instance, the first
order polynomial models the effect of minimising over the template
frequency and frequency derivative.

To calculate the mismatch, we expand Eqn.~\eqref{eqn: mismatch} summing over
the residual frequency offset $\Delta^{(1)}f_i$ (defined in Eqn.~\eqref{eqn:
D2f}) and the residual phase offset $\Delta^{(2)}\phi_i$ (given by
subsituting Eqn.~\eqref{eqn: P2F} into Eqn.~\eqref{eqn: D2phi}), this gives
\begin{align}
\begin{split}
E[\mutilde] = &
\s{i=1}{\Nsd}\left(g_{00}^{E}E\left[\Delta^{(2)}\phi_{i}\Delta^{(2)}\phi_{i}\right]
+ 2 g_{01}^{E}E\left[\Delta^{(2)}\phi_{i}\Delta^{(1)}\f_{i}\right]
+  g_{11}^{E} E\left[\Delta^{(2)}\f_{i}\Delta^{(1)}\f_{i}\right] \right) \\
& + 2\s{i=1}{\Nsd}\s{j=1}{i-1}\left(\right.
g_{00}^{NE}E\left[\Delta^{(2)}\phi_{i}\Delta^{(2)}\phi_{j}\right] +
g_{01}^{NE}E\left[\Delta^{(2)}\phi_{j}\Delta^{(2)}\f_{i}\right] +  \\
&\hspace{20mm}\left. g_{10}^{NE}E\left[\Delta^{(2)}\phi_{i}\Delta^{(2)}\f_{j}\right] +
g_{11}^{NE} E\left[\Delta^{(1)}\f_{i}\Delta^{(1)}\f_{j}\right] \right).
\end{split}
\end{align}


We calculate each of these expressions in a similar manner to Eqn.~\eqref{eqn:
expected mismatch dP0idP0j_k2} replacing the relevant expectations with those
given in Eqn.~\eqref{eqn: E1} to Eqn.~\eqref{eqn: E3}. This yields an expected
mismatch given by
\begin{equation}
E[\mutilde] = \frac{\pi^{2} }{630} \sigF \dT^{2}  \left(\Nsd^{3} + 13\Nsd + \frac{82}{\Nsd} \right).
\label{eqn: Expected mismatch RW in frequency k2}
\end{equation}
This can be compared with the frequency noise term alone in Eqn.~\eqref{eqn:
expectation}. We note that the leading order power remains unchanged, but there
is a reduction in the coefficient and a difference in the second highest
power. The reduction in the coefficient is expected since we have minimised
the mismatch; the change in the second highest power is not yet understood.

\subsection{Verification}

We now verify Eqn.~\eqref{eqn: Expected mismatch RW in frequency k2} and
Eqn.~\eqref{eqn: Expected mismatch RW in phase k2} by comparing with Monte
Carlo simulations. The numerical signals undergo a random walk as described in
Section~\ref{sec: random walk models part I}, however, when searching for the
signals we search over a grid of points in $f_\textrm{t}$ and
$\dot{f}_\textrm{t}$ then select grid point with the minimum mismatch; this
minimises the mismatch over the frequency and spin-down. The results are
plotted in Figure~\ref{fig: verification of minimised RW} and demonstrate good
agreement between the analytic prediction and the mean of the simulated
mismatches.

\begin{figure}[ht]
\centering
\subfloat[Random walk in phase]{\includegraphics[width=0.5\textwidth]{ExpectationPhase_NarrowBand}}
\subfloat[Random walk in frequency]{\includegraphics[width=0.5\textwidth]{ExpectationFrequency_NarrowBand}}\\
%\subfloat[Random walk in spin-down]{\includegraphics[width=0.5\textwidth]{ExpectationSpindown}}
\caption{A comparison of the Monte Carlo numerical simulated mismatch with the
predictions of Eqn.~\eqref{eqn: Expected mismatch RW in
frequency k2} and Eqn.~\eqref{eqn: Expected mismatch RW in phase k2}; this differs
from Figure~\ref{fig: rw I} in that the numerical mismatch is minimised by selecting
the smallest mismatch from a grid of points in $f_\textrm{t}$ and $\dot{f}_\textrm{t}$.}
\label{fig: verification of minimised RW}
\end{figure}

\section{Application to the Crab pulsar}
\label{sec: application to crab}
\meta{
In Section~\ref{sec: timing noise as described by the crab ephemeris}, we
introduced the Crab ephemeris.  The regular and independent measurements of the
frequency and spin-down rate in the Crab ephemeris provide a unique view of
timing noise as a `jump' in the phase, frequency, and spin-down rate each
month. Specifically by a jump we mean the discontinuity at the interface
between two months as illustrated in Figure~\ref{fig: template jumps}. In this
section, we will intepret data from the ephemeris in the context of a random
walk model. In the previous section, we derived the minimum fully-coherent
metric-mismatch due to random walk in the phase or frequency, given the
properties of the underlying random walk. We will therefor finish this section
by making some estimates using the data from the Crab ephemeris which can be
compared with the empirical description discussed in Section~\ref{sec: averaged
mismatch as a function of the observation duration}.Note that in this section,
we discus the rotational frequency and frequency derivatives, which we denote
by $\nu$ and $\dot{\nu}$ which we will relate to the the gravitational wave
frequency $f$ using the non-axisymmetric emmision model for which $f=2\nu$.

\subsection{Distribution of jumps in the Crab ephemeris}
\label{sec: jumps}
\newcommand{\nuddotav}{\ddot{\nu}_{\textrm{av}}}
We begin with a purely empirical look at the distribution of jumps in the
Crab ephemeris. We will use these results in the next section to intepret the
Crab ephemeris in the context of a random walk.

From the Crab ephemeris, there are three distributions which we will calculate
here: the jumps in frequency, spin-down rate, and phase. It is not meaningful
to simply look at the difference in frequency (for example) between any two
months, what we want is the difference which occurs at the interface. To
calculate this we define $\nu_{i}(t)$ as the frequency according to the
$i^{th}$ month as evaluated at time $t$. Then if $\Delta t_{i} = t_{i+1} -
t_{i}$ the frequency jump between months is
\begin{align}
\delta\nu_{i} &= \nu_{i+1}\left(t_{i+1}-\Delta t_{i}/2\right) -  \nu_{i}\left(t_{i}+\Delta t_{i}/2\right), \\
    &= \left[\nu_{i+1}- \frac{\Delta t_{i}}{2}\dot{\nu}_{i+1} + \left(\frac{\Delta t_{i}}{2}\right)^{2}\frac{\ddot{\nu}_{i+1}}{2}\right]
     - \left[\nu_{i} + \frac{\Delta t_{i}}{2}\dot{\nu}_{i}+ \left(\frac{\Delta t_{i}}{2}\right)^{2}\frac{\ddot{\nu}_{i}}{2}\right] .
\end{align}
The Crab ephemeris does not include the second order spin-down, but since the
first order spin-down is not constant we assume a constant average second order
spin-down given by
\begin{equation}
   \nuddotav = \frac{1}{N}\sum_{i} \frac{\dot{\nu}_{i+1} - \dot{\nu_{i}}}{t_{i+1} - t_{i}},
   \label{eqn: average second order spin-down}
\end{equation}
where $N$ is in the number of data points in the ephemeris file. Inserting this
into the Taylor expansion the second order terms cancel leaving a frequency
jump between months given by
\begin{equation}
\delta\nu_{i} = \left(\nu_{i+1}- \nu_{i}\right) -  \left(\dot{\nu}_{i+1}
               + \dot{\nu}_{i}\right)\frac{\Delta t_{i}}{2}.
\label{eqn: crab frequency residuals}
\end{equation}
Note, this is not the difference in frequency between monthly
updates, but the jump between months in frequency at the interface as illustrated in figure
\ref{fig: template jumps}.

Calculating the result of Eqn.~\eqref{eqn: crab frequency residuals} for all
the data points in the Crab ephemeris we plot the estimated density using the
Gaussian kernel density estimate (KDE) method \citep{Scipy} in Figure~\ref{fig:
crab kde}A. Note that we have filtered out differences which occur over known
glitches as described by \citet{Espinoza2011}. This is because we are
interested in the timing noise activity and not the effect of glitches
themselves.
\begin{figure}[ht]
\centering
\includegraphics[]{CrabTN_KDEs.pdf}
\caption{KDEs for the `jumps' in frequency, spin-down rate, and residual number
of rotations between adjacent per-month signals in the Crab ephemeris.
Adjacent signals over glitches are filtered along with large anomalous values
of $\delta N$ once it was confirmed they occur within 200 days after a glitch.}
\label{fig: crab kde}
\end{figure}

Moving now to the spindown, we can calculate the jump between months in a similar way
\begin{align}
\delta\dot{\nu}_{i} & = \dot{\nu}_{i+1}\left(t_{i+1}-\Delta t_{i}/2\right) -  \dot{\nu}_{i}\left(t_{i}+\Delta t_{i}/2\right), \\
& = \left(\dot{\nu}_{i+1}-\dot{\nu}_{i}\right) -  \nuddotav \Delta t_{i}
\label{eqn: crab spindown residuals}
\end{align}
For the spindown rate a population of large negative jumps was observed, these are
found to occur in the post-glitch periods and hence are not the timing noise
activity that we are interested in. As such in Figure~\ref{fig: crab kde}B, we
filter out these anomalous results to show the KDE for data points known to be
related to timing noise.

For the phase we can use that each reference time given in the ephemeris
coincides with a pulse. Therefore between two adjacent references times, the
star has undergone an integer number of rotations. We can evaluate how well the
ephemeris performs here by calculating the residual number of rotations between
the timing model at a given step and the pulse arrival time at the next step.
The data in the ephemeris file does not directly provide information on the
phase evolution, it provides the independent phase evolution in each month with
the phase at the reference time being zero. To calculate the full phase
evolution between two reference times, we need to calculate the phase
difference between each reference time and the interface time between them.
Take this interface to be halfway between such that $\tmid=(t_{i} +
t_{i+1})/2$, then the total number of rotations between two reference times is
\begin{equation}
    N = \frac{1}{2\pi}\left(\left(\phi_{i}(\tmid) - \phi_{i}(t_{i})\right) -
    \left(\phi_{i_i+1}(\tmid) - \phi_{i+1}(t_{i+1})\right)\right)
\end{equation}
The phase at the reference time is zero so this leaves $N =
\frac{1}{2\pi}\left(\phi_{i}(\tmid) - \phi_{i+1}(\tmid)\right)$ where the terms
are explicitly given by
\begin{align}
\phi_{i}(\tmid) & = 2\pi\left((\tmid - t_{i})\nu_{i} +  \frac{\dot{\nu}_{i}}{2!}(\tmid - t_{i})^{2}+\frac{\nuddotav}{3!}(\tmid - t_{i})^{3}\right) \\
\phi_{i+1}(\tmid) & = 2\pi\left((\tmid - t_{i+1})\nu_{i+1} +  \frac{\dot{\nu}_{i+1}}{2!}(\tmid - t_{i+1})^{2}+\frac{\nuddotav}{3!}(\tmid - t_{i+1})^{3}\right) .
\end{align}

The total number of rotations $N$ between months is a function of
the length of a given month and the spindown parameters. Calculating this
allows us to check how well phase-connected lines of the ephemeris are,
we quantify this by the residual number of rotations, defined as
\begin{equation}
\delta N = N - \textrm{round}(N),
\end{equation}
where by `round` we indicate rounding to the nearest integer number of rotations.

In Figure~\ref{fig: crab kde}C we plot the Gaussian KDE of $\Delta N$ having filtered
against known glitch events. We found four jumps where $0.1 < \Delta N < 1.0$;
again these were found to be occur within~$\sim200$~days of known glitches and
so were removed to focus attention on the timing noise activity.

Figure~\ref{fig: crab kde} shows that the distribution of jumps in the frequency,
spin-down rate, and residual number of rotations is centered on zero, as
expected. The interesting part to note here is the size of the standard deviations,
these can potentially be used to test timing noise models, in the next section
we intepret these in the context of a random walk.

\subsection{Interpretating jumps in the Crab ephemeris: random walk models}
\label{sec: crab RW}
%Many of the features used to distinguish the source of timing noise, for
%example the state switching, occur over timescales much longer than typical
%GW searches. The observation of \citet{Hobbs2010} that over sufficiently
%long timescales residuals tend to admit quasi-periodic features can be
%turned around to argue that over sufficiently short timescales timing noise
%is approximated by a random walk. For our purposes then it is sufficient
%to think of timing noise as a random walk.

In this section, we will relate the Crab ephemeris data to a random walk model
as first introduced in Section~\ref{sec: TN interpretations random walk models}.
The methods used parallel the work of \citet{Cordes1980} and \citet{Groth1975},
but are described here for as there are some subtleties to our analysis.

Consider a parameter $\Delta X(t) \in [\Delta\phi, \Delta\nu,\Delta\dot{\nu}]$ being
the difference between the real noisy signal and the search template. $\Delta
X(t)$ then encodes the deviations due to timing noise without concerning the
secular spindown.  We will model timing noise by allowing $\Delta X(t)$ to
undergo a \emph{compound Poisson process}: a random walk where events are
Poisson distributed in time occurring with a rate $\lambda$, but the size the
events are drawn from a normal distribution with zero mean and a variance
$\langle \Delta X^{2} \rangle$, or more formally
\begin{align}
\Delta X(t) = \sum_{i=1}^{N(t)} D_i^{X},
\end{align}
where $\{ N(t): t \ge 0\}$ is a Poisson process with rate $\lambda$ and
$\{D_i^X: i \ge 1\}$ are independent and identically distributed with 
$D_i^X \sim N(0, \langle (D^X)^{2}\rangle)$. Note that $\langle (D^X)^{2} \rangle$
where $X \in [\phi, \nu, \dot{\nu}]$ is the variance of the random walk jumps.


If initially $\Delta X(0)=0$, after an observation time $\dT$  the
expectation and variance can be calculated using Wald's Eqn.~\citep{wald1944cumulative}
and the law of total variance \citep{weiss2006course}
\begin{align}
\textrm{E}[\Delta X(\dT)] & = 0\\
\textrm{Var}[\Delta X(\dT)] &  = \lambda \dT \langle (D^X)^{2}\rangle.\\
\end{align}

If we label the parameter offset measured for blocks of data $\dT$ as  $\Delta
X_{i}(\dT)$ then $\{\Delta X_{1}(\dT), \Delta X_{2}(\dT), \dots\}$ is a
sequence of independent and identically distributed random variables. In
Figure~\ref{fig: crab kde}, we looked at the difference between adjacent
blocks of data which, in this random walk model, would be given by
\begin{align}
\delta X_{i}(\dT) =  \Delta X_{i}(\dT) - \Delta X_{i-1}(\dT)
\end{align}
Since the random walks themselves are defined as summations, the difference
between them must itself be a random walk. Therefore,
\begin{align}
\textrm{E}[\delta X(\dT)] & = 0
\label{eqn: Poisson expectation} \\
\textrm{Var}[\delta X(\dT)] &  = \lambda \dT \langle (D^X)^{2}\rangle.
\label{eqn: Poisson variance}
\end{align}

Let us define $\mu_{X}(\dT)$ and $\sigma_X(\dT)$ as the mean and
standard-deviation of the sequence of differences $\delta X_{i}(\dT)$.  This is
precisely the values given in the titles of Figure~\ref{fig: crab kde} for the
frequency, spin-down rate, and residual number of rotations.
Then if we assume that the measured values of $\delta X_i(\dT)$ are the result of a
random walk, then when we can equate the measured mean and variance with the
predictions of Eqn.~\eqref{eqn: Poisson expectation} and Eqn.~\eqref{eqn:
Poisson variance}. In all three instances, we do find a mean which is close to
zero when compared to the size of the standard deviation. Moreover, in much the
same way that was done by \citet{Cordes1980}, we can calculate a strength
parameter (for details see Section~\ref{sec: TN interpretations random walk
models}). These are given by
\begin{align}
S_{\mathrm{PN}} & = \lambda \langle (D^N)^{2} \rangle = \frac{\sigma_N(\dT)}{\Delta T}, \\
S_{\mathrm{FN}} & = \lambda \langle (D^\nu)^{2} \rangle = \frac{\sigma_\nu(\dT)}{\Delta T},
\label{eqn: SFN} \\
S_{\mathrm{SN}} & = \lambda \langle (D^{\dot{\nu}})^{2} \rangle = 
\frac{\sigma_{\dot{\nu}}(\dT)}{\Delta T}.
\end{align}

From the standard-deviations given in Figure~\ref{fig: crab kde} we can
estimate the strength of each type of timing noise. In order to test the
hypothesis that the variations seen in the ephemeris really are due to a random
walk model, we could need to show that the strengths were invariant to changes
in $\dT$ However, our data is fixed in blocks of length $\dT = 1$~month by the
fact that we are using the Crab ephemeris. Due to this, we cannot test the
hypothesis, but we can compare our computed strength with that given in
the literature.

Studies of the Crab pulsar consistently find that the random walk noise is
best fit by a frequency like noise process. In Table~\ref{tab: SFN lit} we list
the strengths reported from three works on the issue; this is by no means a
complete list, but shows the typical values and spread.
\begin{table}[htb]
\centering
\begin{tabular}{c|c}
 & $S_{\textrm{FN}}$ Hz$^{2}$/s\\ \hline
\citet{Boynton1972} & $0.9\times10^{-22}$\\
\citet{Groth1975} & $ 0.53\times10^{-22}$\\
\citet{Cordes1980} & $0.66\times10^{-22}$
\end{tabular}
\caption{Values for the strength of frequency noise in the Crab pulsar found
         in the literature: all three authors demonstrated that this strength
         was robust to changes in $\dT$, the time which the data is divided into
         to calculate statistical quantities.}
\label{tab: SFN lit}
\end{table}

We estimate the strength of frequency-like noise in the Crab by substituing
$\sigma_{\nu}=1.8\times10^{-8}$~s$^{-1}$ (taken from Figure~\ref{fig: crab
kde}) into Eqn.~\eqref{eqn: SFN} with $\Delta T = 30$~days, this gives
\begin{align}
S_{\mathrm{FN}} = 1.25 \times 10^{-22} \textrm{ Hz}^{2}/\textrm{s}.
\label{eqn: SFN mine}
\end{align}
This strength is of the same order of magnitude as those listed in
Table~\ref{tab: SFN lit}. In the future, it would be interesting to perform a
full literature review and understand how consistent this value is and if
the strength has changed over the $\sim40$~years of observation.

\subsection{Predicting the mismatch in the Crab}
In the previous section we showed how we could estimate the strength of timing
noise in the Crab using the Crab ephemeris and compared these results to those
in the literature. In this section, we will now use the prediction to estimate
the expected mismatch as a function of observation time.

Taking the leading order terms from Eqn.~\eqref{eqn: Expected mismatch RW in
frequency k2}, we can write the expected mismatch in terms of the strength
parameter as
\begin{align}
E[\mutilde] & = \frac{\pi^{2}}{630}\sigma_{f}^{2}\dT^{2}\Nsd^{3} \\
& = \frac{\pi^{2}}{630} S_{\textrm{FN}} \Tobs^{3},
\label{eqn: mu prediction}
\end{align}
from this we see how, when the random walk is considered as a compound Poisson
random walk the mismatch does not depend on $\dT$. In Figure~\ref{fig: mismatch
Tobs} we have already shown the dependence of the mismatch on the observation
time for the Crab. Now we can additionally predict this dependence, given a
value for the strength of frequency noise. This is done in Figure~\ref{fig:
mismatch Tobs update} for two values of the strength parameter.
\begin{figure}[htb]
\centering
\includegraphics[]{Crab_mismatch_Tobs_with_prediction}
\caption{This figure, updated from Figure~\ref{fig: mismatch Tobs}, shows the
dependence of the minimum mismatch on the observation time for the Crab, along
with the prediction of Eqn.~\eqref{eqn: mu prediction} for two values of the
strength parameter: the value $0.53\times10^{-22}$ is that given by \citet{Groth1975}
while the value $1.25\times10^{-22}$ is the value calculated in Eqn.~\eqref{eqn: SFN mine}.}
\label{fig: mismatch Tobs update}
\end{figure}
The prediction using the strength parameter calculated from the Crab ephemeris,
as given in Eqn.~\eqref{eqn: SFN mine}, is remarkably accurate fitting almost
precisely through the means of the data points.


\section{Predicting the mismatch for other pulsars}
\label{sec: other pulsars}
\citet{Jones2004} considered the problem of timing noise in pulsars by
calculating a decoherence time. They then used a fitting formulae from
\citet{Dewey1989} to predict the strength of timing noise and estimate this
decoherence time. The fitting formulae is given by defining the activiy
parameter as the logarithm of the ratio root-mean-square phase residual
to that of the Crab:
\begin{align}
A = \log\left(\frac{\sigma^{2}_{\phi}}{\sigma_{\phi\; \textrm{Crab}}^{2}}\right),
\end{align}
then, the fitting formulae is
\begin{align}
A = -1.4\log P + 0.8 \log \frac{\dot{P}}{10^{-15}} - 3.31
\end{align}
where $P$ is the pulsar's spin period in seconds and $\dot{P}$ is the
dimensionless period derivative.

From Eqn.~\eqref{eqn: S calc} we can convert this into a prediction for the
strength of frequency noise
\begin{align}
S_{\textrm{FN}} = 10^{2A} S_{\textrm{FN}}^{\textrm{Crab}}
\label{eqn: SFN prediction}
\end{align}
Then, using Eqn.~\eqref{eqn: mu prediction} we can predict the level of mismatch
for a gravitational wave search for a pulsar.

Many gravitational wave search may be effected by timing-noise, but blind
searches are particuarly at risk due to the lack of EM information regarding
the timing properties of the star. This includes all-sky searches along with
directed searches where a small patch of sky is searches where it is thought
there may be a neutron star. In Table~\ref{tab: searches} we listed the the
parameter spaces, coherence times and observation times for several recent
blind searches. With the exception of the search Cas A, all of these use an
initial semi-coherent stage. We have not calculated the semi-coherent mismatch
due to a random walk model of timing noise. However, candidates identified in this
semi-coherent stage would be followed by a fully-coherent search. To estimate
how timing noise in the signal may effect this fully-coherent stage, in
Table~\ref{tab: past search RW} we list the \emph{maximum} predicted metric-mismatch
for the observation time and search parameters of each search.
\begin{table}[htb]
\begin{tabular}{l|l|l|l}
\input{\thisdir/past_searches_worst_cases}
\end{tabular}
\caption{Predicted worst-case metric-mismatch due to timing noise for the blind
searches listed in Table~\ref{tab: searches}. These values are calculated by
taking the maximum absolute frequency and spin-down rate searched for,
using Eqn.~\eqref{eqn: SFN prediction} to predict the estimated strength of
timing noise, then calculating the corresponding mismatch from Eqn.~\eqref{eqn:
mu prediction}.}
\label{tab: past search RW}
\end{table}
From this table, we see that for some searches, the predicted metric-mismatch can be
greater than one. This indicates that the true mismatch in such a search would be
large and therefore the signal would be lost. However, we emphasise that these
are worst-case predictions taken at the corner of parameter space where the
mismatch is predicted to be the largest. Moreover, it is unclear if the
fitting formulae of \citet{Dewey1989} is appropriate in these instances. In the
future, we would like to investigate this issue in more detail and understand
how these estimates vary the whole search-parameter space.


\section{Conclusion}

In this Chapter, we have calculated the expectation of the fully-coherent
mismatch when searching for a GW signal which undergoes a random walk in one of
the phase, frequency, or spin-down rate. We did this first for a system in
which the difference between the signal and template was initially zero and
then grew as a random walk. Since this is not a minimised results, we then
demonstrated how to minimised the mismatch with respect to the template
parameters $f_\textrm{t}$ and $\dot{f}_\textrm{t}$. The formulae derived in
this section were verified against Monte-Carlo type simulations of the exact
mismatch.

Following this, we developed our understanding of random walk models in the
context of the data from the Crab ephemeris. We showed that the frequency noise
strength can be measured from this data and has a value consistent with other
results in the literature. Eqn.~\eqref{eqn: mu prediction} is a key result of
this chapter; it predicts the leading order expected mismatch for a search
given the strength of frequency noise and the observation time. We then
demonstrated the excellent agreement of this predictions with the results
found in Chapter~\ref{sec: timing noise in cgw}.

Finally, we gave preliminary estimates for the mismatch due to timing noise
in some typical blind gravitational wave searches. This was done by converting
the signal parameters into the rotation timing parameters, using the \citet{Dewey1989}
fitting formulae to estimate the strength of timing noise, and then using
Eqn.~\eqref{eqn: mu prediction} to predict the levels of mismatch. We
considered worst-case scenarios and found results which indicates there may be
an issue for some all-sky searches.

}

\begin{subappendices}

\section{Summation identities}
\label{sec: summation identities}
In this appendix, we derive two useful summation identities used in
Sec.~\ref{sec: random walk models part I}. First, we have
\begin{align}
\s{b=1}{c}\s{a=1}{b} X_{a} & = \left( X_{1}\right) 
 + \left( X_{1} + X_{2} \right) + \ldots  +\left( X_{1} + X_{2} 
 + \ldots + X_{c-1} + X_{c}\right)\\
& = c X_{1} + (c-1)X_{2} + \ldots + 2 X_{c-1} + X_{c} \\
& = \s{b=1}{c}(c+1-b)X_{b},
\label{eqn: SI 1}
\end{align}
which is used in deriving Eqn.~\eqref{eqn: delta f n}. Second, we have
\begin{align}
\s{j=1}{i-1}\s{k=1}{j-1}(j-k)X_{k} = & [0] + \left[ X_{1}\right] 
+ \left[2X_{1} + X_{2} \right] + \left[3X_{1} + 2X_{2} +X_{3}\right] 
+ \ldots  \nonumber \\
& + \left[\left(i-2\right)X_{1} + \left(i-3\right)X_{2} 
+ \left(i-4\right)X_{3} + \ldots \right. \nonumber \\
& \left. \hspace{5mm}+ 3X_{i-4} + 2X_{i-3} + X_{i-2}\right]  \\
= & \left(1 + 2 + 3 + \ldots +  (i-4) + (i-3) + (i-2)\right)X_{1} \nonumber  \\ 
& + \left(1 + 2 + 3 + \ldots +(i-4) + (i-3) \right)X_{2}  \nonumber \\ 
& + \left(1 + 2 + 3 + \ldots + (i-4) \right)X_{3} + \ldots  \nonumber \\ 
& + (1 + 2 + 3)X_{i-4} + (1+2)X_{i-3} + X_{i-2}  \\
= & \s{k=1}{i-2}k X_{1} + \s{k=1}{i-3}k X_{2}  + \ldots + \s{k=1}{2}k X_{i-3} 
+ \s{k=1}{1}kX_{i-2}  \\
= & \s{j=1}{i-2}\left(\s{k=1}{i-1-j}k\right)X_{j} = 
\frac{1}{2}\s{j=1}{i-2}(i-j)(i-j-1)X_{j},
\label{eqn: SI 2}
\end{align}
which is used in deriving Eqn.~\eqref{eqn: delta phi n}.


\section{Least-squares minimisation of a random walk}
\label{sec: least squares minimisation of a random walk}
In this appendix, we will describe the process of fitting and
removing a polynomial from $N$ data points $(x_i, y_i)$ which undergoes a
random walk. The polynomial will be fitted using a least squares minimisation.
The $x_i$ are the independent points at which $y_i$ (which undergoes a random
walk) is measured. We begin by defining the least-squares fitting method then
go on to calculate the residual for several different degrees of polynomial.
This introduces the method in a generic setting which is then applied in
Section~\ref{sec: random walk models part II} to calculate the mismatch for a
GW signal which undergoes a random walk, but in which the search minimises the
mismatch over the search frequency and frequency derivative.

\subsection{Least squares fitting of a polynomial}
Given $N$ data points $x_{i}$, $y_{i}$, we define the residual from a least-squares
polynomial fit of order $k$, as
\begin{equation}
    r_i^{(k)} = y_{i} - y^{\textrm{(k)}}_{i},
\end{equation}
where
\begin{equation}
y^{\textrm{(k)}}_{i} = a_{0} + a_{1}x_{i} + a_{2}x_{i}^{2} + \dots
                                                           + a_{k} x_{i}^{k},
\end{equation}
is a polynomial of degree $k$.

Then the residual which we want to minimise is
\begin{equation}
R^{2} = \s{i=1}{N}\left(r_i^{(k)}\right)^{2}
      = \s{i=1}{N}\left(y_{i} - \left(a_{0} + a_{1}x_{i} + a_{2}x_{i}^{2} +
        \dots + a_{k} x_{i}^{k}\right)\right)^{2}.
\end{equation}
Partial differentiation with respect to the parameters $a_{i}$, yields $k$
simultaneous equations. Writing these as a matrix and then solving
for the best fit, $\hat{y}^(k)_i$, it can be shown \citep{WolframLeastSquares} that
\begin{align}
\hat{y}^{\textrm{(k)}}_{i} & = X \left(X^{T}X\right)^{-1} X^{T} y_{i} & \textrm{where} & &
X & = \left[\begin{array}{ccccc}
1 & x_{1} & x_{1}^{2} & \dots & x_{1}^{k} \\
1 & x_{2} & x_{2}^{2} & \dots & x_{2}^{k} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_{n} & x_{n}^{2} & \dots & x_{n}^{k} \\
\end{array}\right]
\end{align}
Here $X$ is an example of a \emph{Vandermonde} matrix in which the terms follow
a geometric progression. It is useful to note that
\begin{equation}
XX^{T} = \left[\begin{array}{cccc}
N & \s{i=1}{N}x_{i} & \cdots &  \s{i=1}{N}x_{i}^{k} \\
\s{i=1}{N}x_{i} & \s{i=1}{N}x_{i}^{2} & \cdots &  \s{i=1}{N}x_{i}^{k+1} \\
\vdots & \vdots & \ddots & \vdots \\
\s{i=1}{N}x_{i}^{k} & \s{i=1}{N}x_{i}^{k+1} & \cdots &  \s{i=1}{N}x_{i}^{2k}
\end{array}\right].
\end{equation}

Provided that the $x_{i}$ are suitably defined, then an analytic fit can be
found for any $k$, the difficulty lies in inverting the matrix.

\subsection{Least squares fitting a polynomial to a random walk} We now take
the $x_i, y_i$ to be a Gaussian random walk beginning at the origin. To define
this, let $\delta y_{i} \sim N(0, \sigma^{2})$ be independent and identitically
distributed random variables for which their sum generates the random walk:
\begin{equation}
y_{i} = \sum_{j=1}^{i}\delta y_{i}.
\label{eqn: ToyModel RW definition}
\end{equation}
We also set each random walk event to occur according to $x_{i} = i \Delta x$.
Then the residual after fitting and removing a $k^{th}$ order polynomial to the
random walk $y_i$, is
\begin{equation}
r_i^{(k)} = y_{i} - \hat{y}_{i}^{(k)} = y_{i} - X \left(X^{T}X\right)^{-1} X^{T} y_{i}.
\label{eqn: fitted residual}
\end{equation}
This suggests the residual will be similar to the random walk, but modified by
the least squares fitting.  To illustrate this, in Figure~\ref{fig: ToyModelRW}
we plot a simulated random walk along with several fits.
\begin{figure}[htb]
\centering
\includegraphics[width=.9\textwidth]{ToyModelRW}
\caption{Example of a random walk on the left along with three polynomial fits
of varying order. On the right is the corresponding residual after subtracting
these fits. A dotted line marks the origin in both plots.}
\label{fig: ToyModelRW}
\end{figure}

\subsection{Zeroth order fitting}

We begin with the case of $k=0$ in which $X^{T} = [1, 1, \dots 1]$ such
that
\begin{equation}
X \left(X^{T}X\right)^{-1} X^{T} = \frac{1}{N} J_{N}
\end{equation}
where $J_{N}$ is the $N\times N$ matrix of ones.  Inserting this into
Eqn.~\eqref{eqn: fitted residual}, the residual from a zeroth order fit is
given by
\begin{equation}
r_i^{(0)}= y_{i} - \frac{1}{N} \s{j=1}{N}y_{j}.
\end{equation}
The zeroth order residual can be interpetted as the removing the
average value $\langle y_i \rangle$ from the random walk: this was illustrated
in Figure~\ref{fig: ToyModelRW}.

%For example the
%expectation after $i$ steps of the original random walk can be shown to be
%zero, therefore the expectation for the zeroth order residual after $i$ steps
%will also be zero.

%This can intuitively be understood from the fact that we started out RW at the origin,
%a zeroth order fit shifts the origin but a random walk should

We can now take expectations to understand the behaviour of the residual when
compared to the original definition of the random
walk in Eqn.~\ref{eqn: ToyModel RW definition}. For example, consider
the mean square translation distance from the origin of a random walk after $i$
steps. For a normal random walk, this has the well known result
\begin{equation}
E[y_{i}^{2}] = i \sigma^{2}.
\label{eqn: RW classic}
\end{equation}
We can calculate the corresponding quantity of the $k=0$ residual by first
noting that
\begin{align}
E\left[y_{i}y_{j}\right] & = E\left[\s{k=1}{i}\delta y_{k} \s{l=1}{j}\delta y_{l} \right] \\
& = \s{k=1}{i}\s{l=1}{j}E\left[\delta y_{k} \delta y_{l}\right] \\
& = \s{k=1}{i}\s{l=1}{j} \delta_{kl} \sigma^{2} \\
& = \sigma^{2}\min(i, j),
\label{eqn: E yiyi}
\end{align}
where $\delta_{kl}$ is the Kronecker delta. Then we have
\begin{align}
\left(r^{(0)}_{i}\right)^{2} & = y_{i}^{2} - \frac{2}{N}\s{k=1}{N}y_{i}y_{k} + N^{-2}\s{k=1}{N}\s{l=1}{N}y_{k}y_{l} \\
& =  y_{i}^{2} - 2 N^{-1} \left(\s{k=1}{i}y_{i}y_{k} + \s{k=i+1}{N}y_{i}y_{k} \right)+ N^{-2}\s{k=1}{N}\left(\s{l=1}{k}y_{k}y_{l} + \s{l=k+1}{N}y_{k}y_{l} \right).
\end{align}
Taking the expectation we have
\begin{align}
E\left[\left(r^{(0)}_{i}\right)^{2} \right] & = \sigma^{2}\left(i - \frac{2}{N}\left(\s{k=1}{i}k + \s{k=i+1}{N}i \right)+ \frac{1}{N^{2}}\s{k=1}{N}\left(\s{l=1}{k}l+ \s{l=k+1}{N}k \right) \right) \\
& = \sigma^{2}\left(\frac{N}{3} - i + \frac{1}{2} + \frac{i^{2}}{N} - \frac{i}{N} + \frac{1}{6 N}\right).
\end{align}
This result can be compared with Eqn.~\eqref{eqn: RW classic}, the expectation
of the squared value for a random walk.  In contrast, the expectation after $i$
steps for the residual random walk depends on the length of data $N$ that was
fitted. It can be shown the expectation has a minimum at $i=N/2$.

To further understand the difference between the random walk and
the residual random walk, let us consider the sum of squares after $N$ steps for
the random walk
\begin{equation}
E\left[\s{i=1}{N} y_{i}^{2}\right] = \s{i=1}{N} i \sigma^{2} =
                               \frac{1}{2}\left(N^{2} + N\right)\sigma^{2}.
\label{eqn: sum of squares}
\end{equation}
On the other hand, the sum of squares for the residual random walk is given by
\begin{equation}
E\left[\s{i=1}{N} \left(r^{(0)}_{i}\right)^{2}\right] = 
\frac{1}{6}\left(N^{2} -1\right)\sigma^{2}.
\label{eqn: sum of squares k0}
\end{equation}
Comparing equations \eqref{eqn: sum of squares} and \eqref{eqn: sum of squares
k0} we note that, for the leading order term, the coefficient is reduced, but
the power remains the same.

%We verify this behaviour with a simple script that produces a random walk of
%length $N$ then fits and subtracts a $0^{th}$ order polynimial; we then
%calculate the sum of the square residual. In Figure~\ref{fig:
%sum_of_squares_res_oth_order} we repeat this operation multiple times then plot
%the average of the sum of squares for the residual while varying $N$, the
%prediction of Eqn.~\eqref{eqn: sum of squares k0} is also plotted showing
%agreement.
%
%\begin{figure}[ht]
%\centering
%\includegraphics[width=.6\textwidth]{sum_of_squares_res_oth_order}
%\caption{Comparing Eqn.~\eqref{eqn: sum of squares k0} with the averaged sum of squares for a simulated random walk }
%\label{fig: sum_of_squares_res_oth_order}
%\end{figure}

\subsection{First order fitting}
We now consider a first order fitting for which
\begin{align}
\hat{y}^{(1)}_{i} & = X\left(X^{T}X\right)^{-1} X^{T} y_{i} & \textrm{with} &&
X & = \left[\begin{array}{cc}
1 & \Delta x \\
1 & 2 \Delta x  \\
\vdots & \vdots  \\
1 & N \Delta x  \\
\end{array}\right].
\end{align}
Inserting the definitions of $x_{i}$ we can write
\begin{equation}
\left(X^{T}X\right)^{-1} = \frac{1}{N(N-1)}\left[
\begin{array}{cc}
4N+2 & -\frac{6}{\Delta x} \\
 -\frac{6}{\Delta x} & \frac{12}{\Delta x^{2} (N+1)}
\end{array}•
\right] = \Cone.
\end{equation}
For convenience we have defined a symmetric matrix $\Cone$. We then proceed to
define another matrix
\begin{align}
    \mathcal{C}_{ij}^{(1)} & := X\Cone X^{T} \\  & =
\left[\begin{array}{cc}
1 & \Delta x \\
1 & 2\Delta x  \\
\vdots & \vdots  \\
1 & N \Delta x \\
\end{array}\right]
\left[\begin{array}{cc} \Cone_{11} & \Cone_{12} \\ \Cone_{21} & \Cone_{22} \end{array}\right]
\left[\begin{array}{cccc}
1 & 1 & \dots & 1 \\
\Delta x & 2\Delta x & \dots  & N \Delta x
\end{array}\right] \\
& =
\Cone_{11} J_{N} +
\Cone_{12} \Delta x \left[ \begin{array}{cccc}
2 & 3 & \dots & N+1 \\ 3 & 4 & \dots & \vdots \\ \vdots & & & \\  N+1& \dots & \dots & 2N
\end{array}\right] +
\Cone_{22} \Delta x^{2} \left[ \begin{array}{cccc}
1 & 2 & \dots & N \\ 2 & 4 & \dots & \vdots \\ \vdots & & & \\  N& \dots & \dots & N^{2}
\end{array}\right]
\end{align}
We can write $r^{(1)}_{i}$ as a summation by inferring the dependence of the
$i^{th}$ row of each matrix on the $j^{th}$ column
\begin{align}
r^{(1)}_{i} & = y_{i} - \s{j=1}{N}\mathcal{C}_{ij}^{(1)} y_{j}
& \textrm{ where} &&
\mathcal{C}_{ij}^{(1)} & = \Cone_{11} + \Cone_{12}\Delta x (i+j) + \Cone_{22}\Delta x^{2} ij
\end{align}

We have now defined the first order residual. To understand that fitting an
removing a first order polynomial has, we compute the expectation of the
square for the $i^{th}$ term
\begin{align}
\begin{split}
E\left[\left(r^{(1)}_{i}\right)^{2}\right]  =
\frac{1}{15 N \left(N^{2} - 1\right)} &
\left(2 N^{4} - 18 N^{3} i + 9 N^{3} + 78 N^{2} i^{2} - 78 N^{2} i \right.\\
      &\hspace{3mm} + 14 N^{2} - 120 N i^{3} + 180 N i^{2} - 78 N i \\
      &\hspace{3mm} \left. + 9 N + 60 i^{4} - 120 i^{3} + 78 i^{2} - 18 i + 2\right),
\end{split}
\end{align}
which can be compared with the classic result for a random walk given in
Eqn.~\eqref{eqn: RW classic}. Alternatively, comparing with Eqn.~\eqref{eqn:
sum of squares}, the expected sum of squares for the residual random walk is
\begin{equation}
E\left[\s{i=1}{N} \left(r^{(1)}_{i}\right)^{2}\right] 
= \frac{1}{15}\left(N^{2} -4\right)\sigma^{2},
\label{eqn: sum of squares k1}
\end{equation}
as in the zeroth order fit, the power of the leading order term remains the
same, but the coefficient decreases.

\subsection{Second order fitting}
For the residual left after removing a quadratic, the  argument proceeds in much
the same way with
\begin{align}
r^{(2)}_{i} & = y_{i} - \s{j=1}{N}\mathcal{C}_{ij}^{(2)} y_{j},
\end{align}
where
\begin{align}
\mathcal{C}_{ij}^{(2)}  = &\Ctwo_{11} + \Ctwo_{22}\Delta x^{2}ij +
\Ctwo_{33}\Delta x^{4}i^{2}j^{2} \nonumber + \Ctwo_{12}\Delta x(i+j) \nonumber \\
& + \Ctwo_{13}\Delta x (i^{2} + j^2) + \Ctwo_{23}\Delta x^{3}(ij^{2} + i^{2}j),
\label{eqn: MC_2}
\end{align}
and
\begin{equation}
C^{(2)} = \frac{1}{N(N-1)(N-2)}
\left[\begin{matrix}9 N^{2} + 9 N + 6 & - \frac{1}{\Delta{x}} \left(36 N + 18\right) & \frac{30}{\Delta{x}^{2}}\\- \frac{1}{\Delta{x}} \left(36 N + 18\right) & \frac{12 \left(2 N + 1\right) \left(8 N + 11\right)}{\Delta{x}^{2} \left(N + 1\right) \left(N + 2\right)} & - \frac{180}{\Delta{x}^{3} \left(N + 2\right)}\\\frac{30}{\Delta{x}^{2}} & - \frac{180}{\Delta{x}^{3} \left(N + 2\right)} & \frac{180}{\Delta{x}^{4} \left(N + 1\right) \left(N + 2\right)}\end{matrix}\right].
\label{eqn: C_2}
\end{equation}

The expression for the expected square value is too long to write out in full,
but the expected sum of squares for the residual random walk is
\begin{equation}
E\left[\s{i=1}{N} \left(r^{(2)}_{i}\right)^{2}\right] =
 \frac{1}{70}\left(3N^{2} -27\right)\sigma^{2},
\label{eqn: sum of squares k2}
\end{equation}
for which as in the case of the zeroth order and first order residuals, the
power of the leading order term remains unchanged, but the coefficient
decreases.

\subsection{Conclusions}
\label{sec: appendix conclusions}
We now have a method to calculate statistical quantities from the residual
left over after subtracting a $k^{th}$ order polynomial from a random walk.
Considering the sum of squares for a random walk and the residuals in equations
\eqref{eqn: sum of squares}, \eqref{eqn: sum of squares k0},
\eqref{eqn: sum of squares k1}, and \eqref{eqn: sum of squares k2} we find that
the leading order term retains the same
power of $N$ with increasing $k$ but the coefficient of this power gets
smaller. This reflects the improved fitting with the polynomial degrees. We
also note that with each increase in the order of fit we get a limit
on $N$ for which the sum of squares is positive. For zeroth order fitting this
is $N>1$, for first order $N>2$ and for second order $N>3$. This is because in
order to perform a least squares fit, we need at least $k+1$ points to fit.

\end{subappendices}


\biblio

\end{document}
